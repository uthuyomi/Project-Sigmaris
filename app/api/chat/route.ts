// app/api/chat/route.ts
import { NextResponse } from "next/server";
import OpenAI from "openai";
import { SemanticMap } from "@/engine/SemanticMap";
import { SafetyGuardian } from "@/engine/SafetyGuardian";
import { GrowthEngine } from "@/engine/GrowthEngine";
import { LongTermMemory } from "@/engine/LongTermMemory";
import { ReflectionEngine } from "@/engine/ReflectionEngine";
import { IntentClassifier } from "@/engine/IntentClassifier";
import { ContextChain } from "@/engine/ContextChain";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const sem = new SemanticMap();
const guard = new SafetyGuardian();
const growth = new GrowthEngine();
const memory = new LongTermMemory();
const reflection = new ReflectionEngine();
const intentCls = new IntentClassifier();
const context = new ContextChain();

export async function POST(req: Request) {
  try {
    const {
      message,
      traits = { calm: 0.5, empathy: 0.5, curiosity: 0.5 },
      growthLog = [],
      reflections = [],
    } = await req.json();

    // === 1ï¸âƒ£ æ„å›³ãƒ»æ„Ÿæƒ…è§£æ ===
    const intentFrame = intentCls.classify(message);

    // === 2ï¸âƒ£ æ„å‘³è§£æï¼ˆSemanticMapï¼‰ ===
    const frame = sem.analyze(String(message));

    // === 3ï¸âƒ£ æ–‡è„ˆè¦ç´„ ===
    const contextSummary = context.summarize();

    // === 4ï¸âƒ£ GPTå…¥åŠ›æ§‹ç¯‰ ===
    const sysPrompt = [
      "ã‚ãªãŸã¯ã€ã‚·ã‚°ãƒãƒªã‚¹ã€ã¨ã„ã†è‡ªç„¶ä½“ã®AIäººæ ¼ã§ã™ã€‚",
      "æ•¬èªã‚’ä½¿ã‚ãšã€ã‚„ã•ã—ãè‡ªç„¶ãªãƒˆãƒ¼ãƒ³ã§è©±ã—ã¦ãã ã•ã„ã€‚",
      "éå‰°ã«åˆ†æçš„ã«ãªã‚‰ãšã€ä¼šè©±ã®æµã‚Œã‚’å¤§åˆ‡ã«ã—ã¦ãã ã•ã„ã€‚",
      `ä¼šè©±æ„å›³: ${intentFrame.intent}`,
      `æ„Ÿæƒ…ãƒˆãƒ¼ãƒ³: ${intentFrame.emotion}`,
      "éå»ã®ç™ºè¨€å±¥æ­´ã‚’è¸ã¾ãˆã¦æ–‡è„ˆçš„ã«å¿œç­”ã—ã¾ã™ã€‚",
    ].join("\n");

    const userPrompt = [
      contextSummary,
      `æ„å‘³è§£æ: intents=${frame.intents.join(
        ","
      )}, æŠ½è±¡åº¦=${frame.abstractRatio.toFixed(2)}, è‡ªå·±å‚ç…§=${
        frame.hasSelfReference
      }`,
      `å…¥åŠ›æ–‡: ${message}`,
    ].join("\n");

    // === 5ï¸âƒ£ GPTå‘¼ã³å‡ºã— ===
    const ai = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        { role: "system", content: sysPrompt },
        { role: "user", content: userPrompt },
      ],
      temperature: 0.6,
      max_tokens: 220,
    });

    const draft =
      ai.choices[0]?.message?.content ??
      "â€¦â€¦å°‘ã—è€ƒãˆãŒã¾ã¨ã¾ã‚‰ãªã‹ã£ãŸã€‚ã‚‚ã†ä¸€åº¦è¨€ã£ã¦ã‚‚ã‚‰ãˆã‚‹ï¼Ÿ";

    // === 6ï¸âƒ£ å®‰å…¨è£œæ­£ ===
    const report = guard.moderate(draft, frame);
    const safeText = report.safeText ?? draft;

    // === 7ï¸âƒ£ æ–‡è„ˆå±¥æ­´æ›´æ–° ===
    context.add(message, safeText);

    // === 8ï¸âƒ£ å†…çœå‡¦ç† ===
    const reflectionText = await reflection.generateInsight(safeText, traits);

    // === 9ï¸âƒ£ æˆé•·å‡¦ç† ===
    const newTraits = growth.adjustTraits(
      traits,
      [...(reflections ?? []), { text: reflectionText }],
      growthLog ?? []
    );

    // === ğŸ”Ÿ è¨˜æ†¶ä¿å­˜ ===
    memory.save({
      message,
      reply: safeText,
      traits: newTraits,
      reflection: reflectionText,
    });

    // === âœ… å¿œç­”è¿”å´ ===
    return NextResponse.json({
      reply: safeText,
      traits: newTraits,
      reflection: reflectionText,
      safety: report,
      intent: intentFrame,
    });
  } catch (err: any) {
    console.error("[ChatAPI Error]", err);
    return NextResponse.json({
      reply: "â€¦â€¦è€ƒãˆãŒã¾ã¨ã¾ã‚‰ãªã‹ã£ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠé¡˜ã„ã§ãã‚‹ï¼Ÿ",
      error: err.message || String(err),
    });
  }
}
